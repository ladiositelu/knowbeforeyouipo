## Import all the required libraries

import pandas as pd
import datetime
import sqlite3

import datetime
from bs4 import BeautifulSoup
import requests



import pickle
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
import os
import string
import re
from HTMLParser import HTMLParseError





###Load the dataset containing all the paths to individual companies
data=pd.read_csv('/.../S1_tech_companies.csv')
data['date']=pd.to_datetime(data['date'])
data['year']=data['date'].dt.year # create a year variable
data['conm']=data.conm.str.lower()
data.drop_duplicates(['cik'],inplace=True)
data.reset_index(inplace=True)
print data.shape



def download(data,path):
    '''
        This function takes an SEC file path, downloads the content of the url
        '''
    #docs=[]
    base_url='https://www.sec.gov/Archives/'
    data_dict={}
    for index, row in data.iterrows():
        url=base_url+row['path']
        get_page=requests.get(url).content
        get_page=get_page.lower()#.replace('\n','')
        data_dict[row['conm']]=get_page
        #docs.append(data_dict)
        print str(index) + " has been downloaded and stored"
    return data_dict

documents=download(data, 'path')

def link_search(page,pattern='<b><a '):
    ''' This Function looks for certain patterns that depict a link/seaction header in an S-1
        '''
    indices = []
    idx = 0
    while True:
        idx = page.find(pattern,idx+1)
        if idx==-1:
            break
        indices.append(idx)
    return indices

def link_search_wrapper(page):
    candidate_patterns = ['<b><a ',' link1 ','<b>\n<a ','size=2><b> ']
    for pattern in candidate_patterns:
        indices = link_search(page, pattern)
        if len(indices)>0:
            return indices
    return []


def risk(page, indices):
    ''' This function grabs the sections labeled RISK FACTORS
        '''
    pattern=re.compile(" *[Rr]isk *[Ff]actors *")
    for i in range(len(indices)-1):
        newpage=(page[indices[i]:indices[i]+52])
        for j in re.finditer(pattern,newpage):
            return i

def business(page, indices):
    ''' This function grabs the sections labeled RISK FACTORS
        '''
    pattern=re.compile(" *[Bb]usiness *")
    for i in range(len(indices)-1):
        newpage=(page[indices[i]:indices[i]+52])
        for j in re.finditer(pattern,newpage):
            return i


with open('/Users/oladipoositelu/Desktop/TECH_docs.txt', 'wb') as fp:
    pickle.dump(documents, fp)

risk_factors={}
R_FAIL=[]


risk_count=0
for key, company in documents.iteritems():
    try:
        the_indices=link_search_wrapper(company)
        RISK =risk(company, the_indices)
        RF=company[the_indices[RISK]: the_indices[RISK +1 ]]
        risk_factors[key]=RF
        print str(risk_count) + ' TECH RISK has been downloaded and stored'
        risk_count+=1
    #print str(index) + " TECH RISK has been downloaded and stored"
    except TypeError:
        R_FAIL.append(key)
#print str(index) + " TECH RISK NOT downloaded and stored"




#risk_factors
#techs=[]
##for index,tech in enumerate(risk_factors):
#    try:
#        soup=BeautifulSoup(tech,'html.parser')
#        clean_text=soup.get_text()
#        techs.append(clean_text)
#        print str(index) + " cleaned and appended"
#   except HTMLParseError:
#        print "Document " + str(index) NOT SOUPED
#


#print str(len(risk_factors)) + " companies with risk"
with open('/Users/oladipoositelu/Desktop/TECH_RISK.txt', 'wb') as FP:
    pickle.dump(risk_factors, FP)

business_description={}
b_FAIL=[]
biz_count=0
for key, company in documents.iteritems():
    try:
        the_indices=link_search_wrapper(company)
        biz =business(company, the_indices)
        BZ=company[the_indices[biz]: the_indices[biz +1 ]]
        business_description[key]=BZ
        print str(biz_count) + ' Business has been downloaded and stored'
        biz_count +=1
    #print str(index) + " BUSINESS has been downloaded and stored"
    except TypeError:
        b_FAIL.append(key)
#print str(index) + " BUSINESS NOT downloaded and stored"
# pickle.dump(documents, fp)


print str(len(business_description)) + " companies with business description"

with open('/Users/oladipoositelu/Desktop/TECH_BUSINESS.txt', 'wb') as ff:
    pickle.dump(business_description, ff)
#








